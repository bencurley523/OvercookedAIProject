{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bpcur\\anaconda3\\envs\\PantheonRL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import overcookedgym\n",
    "from stable_baselines3 import PPO\n",
    "from pantheonrl.common.agents import OnPolicyAgent\n",
    "from overcookedgym.overcooked_utils import LAYOUT_LIST\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from pantheonrl.common.agents import StaticPolicyAgent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAYOUT_LIST = ['corridor', 'five_by_five', 'mdp_test', 'multiplayer_schelling',\n",
    "               'random0', 'random1', 'random2', 'random3', 'scenario1_s',\n",
    "               'scenario2', 'scenario2_s', 'scenario3', 'scenario4',\n",
    "               'schelling', 'schelling_s', 'simple', 'simple_single',\n",
    "               'simple_tomato', 'small_corridor', 'unident', 'unident_s']\n",
    "\n",
    "NAME_TRANSLATION = {\n",
    "    \"cramped_room\": \"simple\",\n",
    "    \"asymmetric_advantages\": \"unident_s\",\n",
    "    \"coordination_ring\": \"random1\",\n",
    "    \"forced_coordination\": \"random0\",\n",
    "    \"counter_circuit\": \"random3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env(layout):\n",
    "    environment_name = 'OvercookedMultiEnv-v0'\n",
    "    assert layout in LAYOUT_LIST\n",
    "    env = gym.make(environment_name , layout_name=layout)\n",
    "    mon_env = Monitor(env)\n",
    "    return mon_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoints(PPO_num, cp_array, numcps):\n",
    "    '''\n",
    "    returns a dict of the form {PPO_num: {cp2:a, cp3:b,...}}\n",
    "    this does not include cp1, as it is simply checkpoint 1\n",
    "\n",
    "    numcps includes first and last, so numcps=3 for example will return a dict with cp2 and cp3, cp2 having the reward closest to half of cp3\n",
    "    '''\n",
    "    cp_array = np.append(cp_array, np.zeros((100, numcps)), axis=1)\n",
    "    out = {'PPO_' + str(PPO_num): {}}\n",
    "    max_val = np.max(cp_array[:, 1])\n",
    "    cpvals = {}\n",
    "    for i in range(1, numcps-1):\n",
    "        cpvals['cp' + str(i)] = max_val*i/(numcps-1)\n",
    "    for index, reward in enumerate(cp_array[:, 1]):\n",
    "        for i, val in enumerate(cpvals):\n",
    "            cp_array[index, i+2] = abs(cpvals[val]-reward)\n",
    "        if reward == max_val:\n",
    "            out['PPO_' + str(PPO_num)]['cp' + str(numcps)] = index+1\n",
    "    for i in range(2, numcps):\n",
    "        for index, diff in enumerate(cp_array[:, i]):\n",
    "            if diff == np.min(cp_array[:, i]):\n",
    "                out['PPO_' + str(PPO_num)]['cp' + str(i)] = index+1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(ego_path, partner_path, layout, num_episodes=10):\n",
    "    '''\n",
    "    runs env with specified policies for num_episodes episodes\n",
    "    returns mean episode reward\n",
    "    '''\n",
    "    mon_env = reset_env(layout)\n",
    "    ego = StaticPolicyAgent(PPO.load(ego_path))\n",
    "    partner = StaticPolicyAgent(PPO.load(partner_path))\n",
    "    mon_env.add_partner_agent(partner)\n",
    "    scores = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs = mon_env.reset()\n",
    "        ego_obs = obs\n",
    "        partner_obs = obs\n",
    "        score = 0\n",
    "\n",
    "        for t in range(400):\n",
    "            ego_action = ego.policy.predict(ego_obs)\n",
    "            partner_action = partner.policy.predict(partner_obs)\n",
    "            (ego_obs, partner_obs), (reward1, reward2), done, info = mon_env.multi_step(ego_action[0], partner_action[0])\n",
    "            score += reward1\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO_partner(num_agents, starting_num, layout):\n",
    "    for m in range(starting_num, num_agents+starting_num):\n",
    "        mon_env = reset_env(layout)\n",
    "        partner = OnPolicyAgent(PPO('MlpPolicy', mon_env, verbose=0))\n",
    "        mon_env.add_partner_agent(partner)\n",
    "        mon_env.reset()\n",
    "        PPO_ego = PPO('MlpPolicy', mon_env, verbose=0)\n",
    "        if m<17:\n",
    "            partner_save_path2 = os.path.join('saved_models', 'PPO', layout, '16m6cp', 'pair' + str(m), 'partner', 'checkpoint_1')\n",
    "            ego_save_path2 = os.path.join('saved_models', 'PPO', layout, '16m6cp', 'pair' + str(m), 'ego', 'checkpoint_1')\n",
    "            PPO_ego.save(ego_save_path2)\n",
    "            partner.model.save(partner_save_path2)\n",
    "        if m<33:\n",
    "            partner_save_path = os.path.join('saved_models', 'PPO', layout, '32m3cp', 'pair' + str(m), 'partner', 'checkpoint_1')\n",
    "            ego_save_path = os.path.join('saved_models', 'PPO', layout, '32m3cp', 'pair' + str(m), 'ego', 'checkpoint_1')\n",
    "            PPO_ego.save(ego_save_path)\n",
    "            partner.model.save(partner_save_path)\n",
    "        if m>32:\n",
    "            partner_save_path3 = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(m-32), 'partner', 'checkpoint_1')\n",
    "            ego_save_path3 = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(m-32), 'ego', 'checkpoint_1')\n",
    "            PPO_ego.save(ego_save_path3)\n",
    "            partner.model.save(partner_save_path3)\n",
    "        PPO_ego.save(ego_save_path)\n",
    "        partner.model.save(partner_save_path)\n",
    "        print('saved checkpoint 1 of agent ' + str(m))\n",
    "        for cp in range(2, 101):\n",
    "            PPO_ego.learn(total_timesteps=3000)\n",
    "            if m<33:\n",
    "                partner_save_path = os.path.join('saved_models', 'PPO', layout, '32m3cp', 'pair' + str(m), 'partner', 'checkpoint_' + str(cp))\n",
    "                ego_save_path = os.path.join('saved_models', 'PPO', layout, '32m3cp', 'pair' + str(m), 'ego', 'checkpoint_' + str(cp))\n",
    "                PPO_ego.save(ego_save_path)\n",
    "                partner.model.save(partner_save_path)\n",
    "            if m<17:\n",
    "                partner_save_path2 = os.path.join('saved_models', 'PPO', layout, '16m6cp', 'pair' + str(m), 'partner', 'checkpoint_' + str(cp))\n",
    "                ego_save_path2 = os.path.join('saved_models', 'PPO', layout, '16m6cp', 'pair' + str(m), 'ego', 'checkpoint_' + str(cp))\n",
    "                PPO_ego.save(ego_save_path2)\n",
    "                partner.model.save(partner_save_path2)\n",
    "            if m>32:\n",
    "                partner_save_path3 = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(m-32), 'partner', 'checkpoint_' + str(cp))\n",
    "                ego_save_path3 = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(m-32), 'ego', 'checkpoint_' + str(cp))\n",
    "                PPO_ego.save(ego_save_path3)\n",
    "                partner.model.save(partner_save_path3)\n",
    "            PPO_ego.save(ego_save_path)\n",
    "            partner.model.save(partner_save_path)\n",
    "            print(f\"saved checkpoint {cp} of agent {m}\")\n",
    "        del PPO_ego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_32_partners(numPPO, layout, plot=False):\n",
    "    if plot:\n",
    "        y = []\n",
    "        x = [n for n in range(1, 101)]\n",
    "    cp_array = np.array([np.arange(1, 101)]).reshape(100, 1)\n",
    "    cp_array = np.append(cp_array, np.zeros((100, 1)), axis=1)\n",
    "    for cp in range(1, 101):\n",
    "        partner_save_path = os.path.join('saved_models', 'PPO', layout, '32m3cp', 'pair' + str(numPPO), 'partner', 'checkpoint_' + str(cp))\n",
    "        ego_save_path = os.path.join('saved_models', 'PPO', layout, '32m3cp', 'pair' + str(numPPO), 'ego', 'checkpoint_' + str(cp))\n",
    "        mean_reward = run_env(ego_save_path, partner_save_path, layout, 10)\n",
    "        if plot:\n",
    "            y.append(mean_reward)\n",
    "        cp_array[cp-1, 1] = mean_reward\n",
    "        print('Team ' + str(numPPO) + ', evaluated checkpoint ' + str(cp) + '. mean reward: ' + str(mean_reward))\n",
    "    if plot:\n",
    "        return cp_array, x, y\n",
    "    return cp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_16_partners(numPPO, layout, plot=False):\n",
    "    if plot:\n",
    "        y = []\n",
    "        x = [n for n in range(1, 101)]\n",
    "    cp_array = np.array([np.arange(1, 101)]).reshape(100, 1)\n",
    "    cp_array = np.append(cp_array, np.zeros((100, 1)), axis=1)\n",
    "    for cp in range(1, 101):\n",
    "        partner_save_path = os.path.join('saved_models', 'PPO', layout, '16m6cp', 'pair' + str(numPPO), 'partner', 'checkpoint_' + str(cp))\n",
    "        ego_save_path = os.path.join('saved_models', 'PPO', layout, '16m6cp', 'pair' + str(numPPO), 'ego', 'checkpoint_' + str(cp))\n",
    "        mean_reward = run_env(ego_save_path, partner_save_path, layout, 10)\n",
    "        if plot:\n",
    "            y.append(mean_reward)\n",
    "        cp_array[cp-1, 1] = mean_reward\n",
    "        print('Team ' + str(numPPO) + ', evaluated checkpoint ' + str(cp) + '. mean reward: ' + str(mean_reward))\n",
    "    if plot:\n",
    "        return cp_array, x, y\n",
    "    return cp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eval_partners(numPPO, layout, plot=False):\n",
    "    if plot:\n",
    "        y = []\n",
    "        x = [n for n in range(1, 101)]\n",
    "    cp_array = np.array([np.arange(1, 101)]).reshape(100, 1)\n",
    "    cp_array = np.append(cp_array, np.zeros((100, 1)), axis=1)\n",
    "    for cp in range(1, 101):\n",
    "        partner_save_path = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(numPPO), 'partner', 'checkpoint_' + str(cp))\n",
    "        ego_save_path = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(numPPO), 'ego', 'checkpoint_' + str(cp))\n",
    "        mean_reward = run_env(ego_save_path, partner_save_path, layout, 10)\n",
    "        if plot:\n",
    "            y.append(mean_reward)\n",
    "        cp_array[cp-1, 1] = mean_reward\n",
    "        print('Team ' + str(numPPO) + ', evaluated checkpoint ' + str(cp) + '. mean reward: ' + str(mean_reward))\n",
    "    if plot:\n",
    "        return cp_array, x, y\n",
    "    return cp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_PPO_partners(layout, checkpoints, FCPtype):\n",
    "    mon_env = reset_env(layout)\n",
    "    if FCPtype=='16m6cp':\n",
    "        num_partners = 16\n",
    "    else:\n",
    "        num_partners = 32\n",
    "    for ppo_num in range(1, num_partners+1):\n",
    "        partner = StaticPolicyAgent(PPO.load(os.path.join('saved_models', 'PPO', layout, FCPtype, 'pair' + str(ppo_num), 'partner', 'checkpoint_1'), mon_env).policy)\n",
    "        mon_env.add_partner_agent(partner)\n",
    "        for cp in checkpoints['PPO_' + str(ppo_num)]:\n",
    "            checkpoint = checkpoints['PPO_1'][cp]\n",
    "            partner = StaticPolicyAgent(PPO.load(os.path.join('saved_models', 'PPO', layout, FCPtype, 'pair' + str(ppo_num), 'partner', 'checkpoint_' + str(checkpoint))).policy)\n",
    "            mon_env.add_partner_agent(partner)\n",
    "    return mon_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_for_layout(layout):\n",
    "    '''\n",
    "    trains and evaluates both FCP models for given layout\n",
    "    returns dicts of results, means, and plots in format (results32, means32, results16, means16, plots)\n",
    "    '''\n",
    "    train_PPO_partner(42, 1, layout)\n",
    "    \n",
    "    plots = {}\n",
    "\n",
    "    keeping = {}\n",
    "    numcps = 3\n",
    "\n",
    "    x_plots_32 = []\n",
    "    y_plots_32 = []\n",
    "\n",
    "    for ppo in range(1, 33):\n",
    "        partner_cp_array, x, y = evaluate_32_partners(ppo, layout, True)\n",
    "        keeping.update(find_checkpoints(ppo, partner_cp_array, numcps))\n",
    "        x_plots_32.append(x)\n",
    "        y_plots_32.append(y)\n",
    "\n",
    "    keeping_32 = keeping\n",
    "    plots['32m3cp_partners'] = (x_plots_32, y_plots_32)\n",
    "\n",
    "    keeping = {}\n",
    "    numcps = 6\n",
    "\n",
    "    x_plots_16 = []\n",
    "    y_plots_16 = []\n",
    "\n",
    "    for ppo in range(1, 17):\n",
    "        partner_cp_array, x, y = evaluate_16_partners(ppo, layout, True)\n",
    "        keeping.update(find_checkpoints(ppo, partner_cp_array, numcps))\n",
    "        x_plots_16.append(x)\n",
    "        y_plots_16.append(y)\n",
    "\n",
    "    keeping_16 = keeping\n",
    "    plots['16m6cp_partners'] = (x_plots_16, y_plots_16)\n",
    "\n",
    "    keeping = {}\n",
    "    numcps = 5\n",
    "\n",
    "    x_plots_evals = []\n",
    "    y_plots_evals = []\n",
    "\n",
    "    for ppo in range(1, 11):\n",
    "        partner_cp_array, x, y = evaluate_eval_partners(ppo, layout, True)\n",
    "        keeping.update(find_checkpoints(ppo, partner_cp_array, numcps))\n",
    "        x_plots_evals.append(x)\n",
    "        y_plots_evals.append(y)\n",
    "\n",
    "    keeping_evals = keeping\n",
    "    plots['eval_partners'] = (x_plots_evals, y_plots_evals)\n",
    "\n",
    "    FCPtype = '32m3cp'\n",
    "    mon_env = reset_env(layout)\n",
    "    mon_env = add_PPO_partners(layout, keeping_32, FCPtype)\n",
    "    FCP_32m3cp = PPO('MlpPolicy', mon_env, verbose=0)\n",
    "    eval_save_path = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair1', 'partner', 'checkpoint_' + str(keeping_evals['PPO_1']['cp5']))\n",
    "    evals = np.array([])\n",
    "    total_timesteps = 1500000\n",
    "    num_episodes = int(total_timesteps/400)\n",
    "    for ep in range(num_episodes):\n",
    "        mon_env.reset()\n",
    "        FCP_32m3cp.learn(total_timesteps=400)\n",
    "        if ep%37==0:\n",
    "            FCP_32m3cp_save_path = os.path.join('saved_models', 'FCP', layout, FCPtype, 'checkpoint_' + str(int((ep/37)+1)))\n",
    "            FCP_32m3cp.save(FCP_32m3cp_save_path)\n",
    "            mean_reward = run_env(FCP_32m3cp_save_path, eval_save_path, layout, num_episodes=1)\n",
    "            evals = np.append(evals, mean_reward)\n",
    "            print(f'ep{ep}: {mean_reward}')\n",
    "            mon_env = add_PPO_partners(layout, keeping_32, FCPtype)\n",
    "        print(f'completed episode {ep+1} of {num_episodes} for {FCPtype}')\n",
    "\n",
    "    x_plots_32m3cp = evals\n",
    "    y_plots_32m3cp = np.arange(0, 1500400, 37*400)\n",
    "    plots['FCP_32m3cp'] = (x_plots_32m3cp, y_plots_32m3cp)\n",
    "\n",
    "    FCPtype = '16m6cp'\n",
    "    mon_env = reset_env(layout)\n",
    "    mon_env = add_PPO_partners(layout, keeping_16, FCPtype)\n",
    "    FCP_16m6cp = PPO('MlpPolicy', mon_env, verbose=0)\n",
    "    eval_save_path = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair1', 'partner', 'checkpoint_' + str(keeping_evals['PPO_1']['cp5']))\n",
    "    evals = np.array([])\n",
    "    total_timesteps = 1500000\n",
    "    num_episodes = int(total_timesteps/400)\n",
    "    for ep in range(num_episodes):\n",
    "        mon_env.reset()\n",
    "        FCP_16m6cp.learn(total_timesteps=400)\n",
    "        if ep%37==0:\n",
    "            FCP_16m6cp_save_path = os.path.join('saved_models', 'FCP', layout, FCPtype, 'checkpoint_' + str(int((ep/37)+1)))\n",
    "            FCP_16m6cp.save(FCP_16m6cp_save_path)\n",
    "            mean_reward = run_env(FCP_16m6cp_save_path, eval_save_path, layout, num_episodes=1)\n",
    "            evals = np.append(evals, mean_reward)\n",
    "            print(f'ep{ep}: {mean_reward}')\n",
    "            mon_env = add_PPO_partners(layout, keeping_16, FCPtype)\n",
    "        print(f'completed episode {ep+1} of {num_episodes} for {FCPtype}')\n",
    "\n",
    "    x_plots_16m6cp = evals\n",
    "    y_plots_16m6cp = np.arange(0, 1500400, 37*400)\n",
    "    plots['FCP_16m6cp'] = (x_plots_16m6cp, y_plots_16m6cp)\n",
    "\n",
    "    results_32m3cp = {}\n",
    "    FCP_path = os.path.join('saved_models', 'FCP', layout, '32m3cp', 'checkpoint_102')\n",
    "\n",
    "    for cp in range(1, 6):\n",
    "        results_32m3cp['cp' + str(cp)] = []\n",
    "        for partner_num in range(1, 11):\n",
    "            try:\n",
    "                cp_num = keeping_evals['PPO_' + str(partner_num)]['cp' + str(cp)]\n",
    "            except:\n",
    "                cp_num = 1\n",
    "            partner_path = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(partner_num), 'partner', 'checkpoint_' + str(cp_num))\n",
    "            results_32m3cp['cp' + str(cp)].append(run_env(FCP_path, partner_path, layout, num_episodes=10))\n",
    "\n",
    "    results_16m6cp = {}\n",
    "    FCP_path = os.path.join('saved_models', 'FCP', layout, '16m6cp', 'checkpoint_102')\n",
    "\n",
    "    for cp in range(1, 6):\n",
    "        results_16m6cp['cp' + str(cp)] = []\n",
    "        for partner_num in range(1, 11):\n",
    "            try:\n",
    "                cp_num = keeping_evals['PPO_' + str(partner_num)]['cp' + str(cp)]\n",
    "            except:\n",
    "                cp_num = 1\n",
    "            partner_path = os.path.join('saved_models', 'PPO', layout, 'eval', 'pair' + str(partner_num), 'partner', 'checkpoint_' + str(cp_num))\n",
    "            results_16m6cp['cp' + str(cp)].append(run_env(FCP_path, partner_path, layout, num_episodes=10))\n",
    "\n",
    "    means_32m3cp = {key: np.mean(results_32m3cp[key]) for key in results_32m3cp}\n",
    "    means_16m6cp = {key: np.mean(results_16m6cp[key]) for key in results_16m6cp}\n",
    "\n",
    "    return results_32m3cp, means_32m3cp, results_16m6cp, means_16m6cp, plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MediumLevelPlanner from c:\\users\\bpcur\\pantheonrl\\overcookedgym\\human_aware_rl\\overcooked_ai\\overcooked_ai_py\\data\\planners\\random1_am.pkl\n",
      "Team 1, evaluated checkpoint 1. mean reward: 4.7\n",
      "Loaded MediumLevelPlanner from c:\\users\\bpcur\\pantheonrl\\overcookedgym\\human_aware_rl\\overcooked_ai\\overcooked_ai_py\\data\\planners\\random1_am.pkl\n",
      "Team 1, evaluated checkpoint 2. mean reward: 7.2\n",
      "Loaded MediumLevelPlanner from c:\\users\\bpcur\\pantheonrl\\overcookedgym\\human_aware_rl\\overcooked_ai\\overcooked_ai_py\\data\\planners\\random1_am.pkl\n",
      "Team 1, evaluated checkpoint 3. mean reward: 10.2\n",
      "...",
      "Loaded MediumLevelPlanner from c:\\users\\bpcur\\pantheonrl\\overcookedgym\\human_aware_rl\\overcooked_ai\\overcooked_ai_py\\data\\planners\\simple_am.pkl\n"
     ]
    }
   ],
   "source": [
    "keeping = {}\n",
    "for ppo in range(1, 11):\n",
    "        partner_cp_array = evaluate_eval_partners(ppo, 'random1')\n",
    "        keeping.update(find_checkpoints(ppo, partner_cp_array, 5))\n",
    "keeping_evals_random1 = keeping\n",
    "\n",
    "keeping = {}\n",
    "for ppo in range(1, 11):\n",
    "        partner_cp_array = evaluate_eval_partners(ppo, 'random0')\n",
    "        keeping.update(find_checkpoints(ppo, partner_cp_array, 5))\n",
    "keeping_evals_random0 = keeping\n",
    "\n",
    "results_unident_s = {}\n",
    "ego_path = os.path.join('saved_models', 'PPO', 'unident_s', '16m6cp', 'pair1', 'ego', 'checkpoint_100')\n",
    "\n",
    "for cp in range(1, 6):\n",
    "        results_unident_s['cp' + str(cp)] = []\n",
    "        for partner_num in range(1, 11):\n",
    "                partner_path = os.path.join('saved_models', 'PPO', 'unident_s', 'eval', 'pair' + str(partner_num), 'partner', 'checkpoint_' + str(cp))\n",
    "                results_unident_s['cp' + str(cp)].append(run_env(ego_path, partner_path, 'unident_s', num_episodes=10))\n",
    "\n",
    "results_random1 = {}\n",
    "ego_path = os.path.join('saved_models', 'PPO', 'random1', '16m6cp', 'pair1', 'ego', 'checkpoint_100')\n",
    "\n",
    "for cp in range(1, 6):\n",
    "        results_random1['cp' + str(cp)] = []\n",
    "        for partner_num in range(1, 11):\n",
    "                try:\n",
    "                        cp_num = keeping_evals_random1['PPO_' + str(partner_num)]['cp' + str(cp)]\n",
    "                except:\n",
    "                        cp_num = 1\n",
    "                partner_path = os.path.join('saved_models', 'PPO', 'random1', 'eval', 'pair' + str(partner_num), 'partner', 'checkpoint_' + str(cp_num))\n",
    "                results_random1['cp' + str(cp)].append(run_env(ego_path, partner_path, 'random1', num_episodes=10))\n",
    "\n",
    "results_random0 = {}\n",
    "ego_path = os.path.join('saved_models', 'PPO', 'random0', '16m6cp', 'pair1', 'ego', 'checkpoint_100')\n",
    "\n",
    "for cp in range(1, 6):\n",
    "        results_random0['cp' + str(cp)] = []\n",
    "        for partner_num in range(1, 11):\n",
    "                try:\n",
    "                        cp_num = keeping_evals_random0['PPO_' + str(partner_num)]['cp' + str(cp)]\n",
    "                except:\n",
    "                        cp_num = 1\n",
    "                partner_path = os.path.join('saved_models', 'PPO', 'random0', 'eval', 'pair' + str(partner_num), 'partner', 'checkpoint_' + str(cp_num))\n",
    "                results_random0['cp' + str(cp)].append(run_env(ego_path, partner_path, 'random0', num_episodes=10))\n",
    "\n",
    "results_simple = {}\n",
    "ego_path = os.path.join('saved_models', 'PPO', 'simple', '16m6cp', 'pair1', 'ego', 'checkpoint_100')\n",
    "\n",
    "for cp in range(1, 6):\n",
    "        results_simple['cp' + str(cp)] = []\n",
    "        for partner_num in range(1, 11):\n",
    "                try:\n",
    "                        cp_num = keeping_evals_simple['PPO_' + str(partner_num)]['cp' + str(cp)]\n",
    "                except:\n",
    "                        cp_num = 1\n",
    "                partner_path = os.path.join('saved_models', 'PPO', 'simple', 'eval', 'pair' + str(partner_num), 'partner', 'checkpoint_' + str(cp_num))\n",
    "                results_simple['cp' + str(cp)].append(run_env(ego_path, partner_path, 'simple', num_episodes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cp1': 8.900000000000002, 'cp2': 57.48, 'cp3': 85.42999999999999, 'cp4': 120.8, 'cp5': 136.41000000000003}\n"
     ]
    }
   ],
   "source": [
    "means_simple = {key: np.mean(results_simple[key]) for key in results_simple}\n",
    "print(means_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cp1': 5.27, 'cp2': 8.41, 'cp3': 19.05, 'cp4': 34.690000000000005, 'cp5': 46.07000000000001}\n"
     ]
    }
   ],
   "source": [
    "means_random0 = {key: np.mean(results_random0[key]) for key in results_random0}\n",
    "print(means_random0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cp1': 50.33, 'cp2': 101.51, 'cp3': 83.46000000000001, 'cp4': 81.29, 'cp5': 81.76000000000002}\n"
     ]
    }
   ],
   "source": [
    "means_random1 = {key: np.mean(results_random1[key]) for key in results_random1}\n",
    "print(means_random1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cp1': 31.27, 'cp2': 114.46, 'cp3': 233.93999999999997, 'cp4': 332.03999999999996, 'cp5': 418.35}\n"
     ]
    }
   ],
   "source": [
    "means_unident_s = {key: np.mean(results_unident_s[key]) for key in results_unident_s}\n",
    "print(means_unident_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymmetric Advantages Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_32m3cp_unident_s = {'cp1': [35.8, 36.2, 43.4, 39.0, 29.4, 25.9, 31.1, 25.9, 31.9, 36.5],\n",
    " 'cp2': [132.2, 96.4, 83.6, 107.8, 93.1, 126.9, 130.3, 94.9, 113.1, 118.1],\n",
    " 'cp3': [241.9, 190.8, 235.6, 181.0, 216.6, 203.8, 194.9, 231.4, 200.3, 200.3],\n",
    " 'cp4': [293.8, 325.2, 314.9, 266.1, 322.9, 310.3, 298.8, 296.8, 297.5, 268.5],\n",
    " 'cp5': [403.3, 395.7, 380.3, 384.4, 380.9, 392.3, 372.0, 377.1, 387.2, 355.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_16m6cp_unident_s = {'cp1': [28.1, 20.4, 25.4, 13.7, 44.5, 33.2, 18.2, 34.1, 25.9, 32.2],\n",
    " 'cp2': [119.4, 90.9, 109.6, 122.3, 77.0, 109.0, 112.7, 106.7, 104.5, 104.0],\n",
    " 'cp3': [227.1, 228.8, 223.3, 207.9, 227.5, 227.5, 182.1, 197.4, 186.1, 174.0],\n",
    " 'cp4': [286.4, 341.0, 315.6, 296.4, 307.3, 281.1, 295.1, 291.7, 297.5, 255.6],\n",
    " 'cp5': [402.6, 393.7, 389.0, 379.8, 396.7, 392.1, 379.8, 359.1, 391.8, 348.4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_16m6cp_unident_s = {key: np.mean(results_16m6cp_unident_s[key]) for key in results_16m6cp_unident_s}\n",
    "means_32m3cp_unident_s = {key: np.mean(results_32m3cp_unident_s[key]) for key in results_32m3cp_unident_s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cp1': 27.57,\n",
       " 'cp2': 105.60999999999999,\n",
       " 'cp3': 208.17,\n",
       " 'cp4': 296.77000000000004,\n",
       " 'cp5': 383.30000000000007}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_16m6cp_unident_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cp1': 33.51,\n",
       " 'cp2': 109.64000000000001,\n",
       " 'cp3': 209.66,\n",
       " 'cp4': 299.48,\n",
       " 'cp5': 382.90999999999997}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_32m3cp_unident_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_unident_s = {'cp1': [34.7, 36.9, 25.6, 35.4, 21.1, 25.3, 35.8, 20.5, 35.4, 42.0],\n",
    " 'cp2': [139.1, 85.6, 100.5, 145.4, 93.2, 134.7, 129.8, 98.6, 118.1, 99.6],\n",
    " 'cp3': [240.0, 256.0, 269.0, 184.1, 241.1, 253.8, 206.3, 268.4, 213.7, 207.0],\n",
    " 'cp4': [334.4, 362.1, 340.8, 333.0, 368.6, 325.6, 336.9, 338.5, 323.6, 256.9],\n",
    " 'cp5': [443.8, 429.7, 434.4, 431.3, 423.9, 445.7, 398.4, 402.2, 426.5, 347.6]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramped Room Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_32m3cp_simple = {'cp1': [278.5, 279.6, 291.0, 287.5, 286.6, 285.5, 288.8, 276.3, 266.9, 290.3],\n",
    " 'cp2': [269.0, 271.3, 302.5, 255.8, 258.0, 267.4, 273.7, 271.2, 271.8, 292.2],\n",
    " 'cp3': [271.2, 258.7, 307.7, 245.8, 263.7, 299.2, 289.4, 267.4, 254.3, 324.5],\n",
    " 'cp4': [241.1, 281.6, 352.7, 261.1, 249.7, 371.0, 305.3, 264.1, 213.2, 370.4],\n",
    " 'cp5': [201.3, 209.9, 364.7, 247.0, 183.3, 383.9, 363.8, 286.4, 140.0, 392.6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_16m6cp_simple = {'cp1': [293.6, 287.2, 289.1, 308.4, 298.0, 296.2, 287.4, 282.0, 296.1, 297.4],\n",
    " 'cp2': [255.3, 272.4, 293.2, 233.7, 283.2, 274.7, 257.0, 283.6, 267.9, 287.1],\n",
    " 'cp3': [260.7, 296.5, 286.3, 221.5, 267.4, 316.3, 282.8, 278.3, 237.1, 258.8],\n",
    " 'cp4': [257.2, 296.3, 301.5, 202.7, 252.3, 335.1, 269.6, 290.7, 219.4, 364.7],\n",
    " 'cp5': [254.1, 278.5, 293.2, 193.5, 196.8, 319.2, 284.2, 299.5, 132.5, 414.7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_16m6cp_simple = {'cp1': 293.53999999999996,\n",
    " 'cp2': 270.81,\n",
    " 'cp3': 270.57000000000005,\n",
    " 'cp4': 278.95,\n",
    " 'cp5': 266.62}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_32m3cp_simple = {'cp1': 283.1,\n",
    " 'cp2': 273.28999999999996,\n",
    " 'cp3': 278.19,\n",
    " 'cp4': 291.02000000000004,\n",
    " 'cp5': 277.29}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_simple = {'cp1': [9.6, 5.4, 8.2, 6.3, 7.9, 6.8, 10.0, 11.0, 10.4, 13.4],\n",
    " 'cp2': [17.1, 17.6, 107.3, 19.8, 17.1, 120.6, 68.8, 43.2, 0.9, 162.4],\n",
    " 'cp3': [7.2, 36.0, 105.5, 10.7, 19.3, 169.5, 170.0, 71.8, 0.0, 264.3],\n",
    " 'cp4': [9.3, 30.3, 160.1, 4.2, 1.8, 287.2, 242.7, 125.5, 0.0, 346.9],\n",
    " 'cp5': [0.9, 24.7, 264.1, 1.2, 0.6, 326.0, 300.1, 70.2, 0.0, 376.3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced Coordination Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_32m3cp_random0 = {'cp1': [14.5, 13.2, 11.8, 12.9, 14.5, 11.3, 18.8, 8.7, 13.0, 11.2],\n",
    " 'cp2': [188.8, 118.7, 97.9, 136.4, 156.7, 78.0, 113.2, 167.6, 59.1, 129.4],\n",
    " 'cp3': [264.2, 252.4, 187.6, 226.6, 160.2, 135.4, 259.8, 191.7, 84.1, 235.9],\n",
    " 'cp4': [338.2, 282.1, 292.3, 278.8, 282.8, 144.3, 318.9, 258.9, 88.5, 297.5],\n",
    " 'cp5': [362.2, 325.8, 360.7, 318.0, 291.6, 161.6, 300.1, 314.7, 147.9, 343.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_16m6cp_random0 = {'cp1': [16.1, 25.7, 20.8, 19.5, 11.2, 17.3, 11.1, 8.0, 8.1, 12.1],\n",
    " 'cp2': [178.4, 141.4, 148.5, 127.6, 161.3, 75.0, 174.3, 161.6, 70.0, 108.6],\n",
    " 'cp3': [287.7, 279.9, 269.6, 178.2, 197.3, 153.6, 276.7, 244.9, 75.1, 257.8],\n",
    " 'cp4': [300.8, 250.1, 314.8, 201.1, 281.6, 179.9, 307.3, 310.2, 78.2, 304.8],\n",
    " 'cp5': [329.1, 236.2, 381.5, 312.4, 227.7, 160.4, 340.5, 314.2, 109.7, 324.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_32m3cp_random0 = {'cp1': 12.99,\n",
    " 'cp2': 124.58,\n",
    " 'cp3': 199.79000000000002,\n",
    " 'cp4': 258.23,\n",
    " 'cp5': 292.61}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_16m6cp_random0 = {'cp1': 14.989999999999998,\n",
    " 'cp2': 134.67000000000002,\n",
    " 'cp3': 222.07999999999998,\n",
    " 'cp4': 252.88000000000002,\n",
    " 'cp5': 273.58}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_random0 = {'cp1': [4.8, 4.8, 5.7, 6.4, 8.5, 2.7, 6.0, 5.7, 4.8, 3.3],\n",
    " 'cp2': [0.0, 1.2, 0.0, 0.3, 2.7, 14.4, 0.0, 0.0, 64.0, 1.5],\n",
    " 'cp3': [0.0, 0.0, 0.0, 0.3, 3.3, 63.9, 0.0, 0.3, 122.7, 0.0],\n",
    " 'cp4': [0.3, 0.6, 0.0, 0.0, 2.1, 158.8, 0.0, 0.3, 184.8, 0.0],\n",
    " 'cp5': [0.0, 0.0, 0.0, 0.0, 0.0, 211.3, 0.0, 0.0, 249.4, 0.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordination Ring Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_32m3cp_random1 = {'cp1': [161.7, 183.0, 182.4, 184.2, 183.7, 174.8, 187.2, 176.2, 180.0, 190.6],\n",
    " 'cp2': [271.6, 270.6, 233.9, 219.6, 200.2, 166.5, 215.1, 207.0, 213.4, 222.6],\n",
    " 'cp3': [274.5, 291.1, 222.7, 209.3, 169.3, 166.3, 196.2, 243.6, 194.2, 209.3],\n",
    " 'cp4': [246.2, 283.1, 231.6, 168.2, 134.6, 194.0, 254.1, 253.8, 202.6, 243.6],\n",
    " 'cp5': [278.5, 266.0, 204.0, 197.2, 182.5, 219.1, 264.2, 215.4, 161.0, 224.7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_16m6cp_random1 = {'cp1': [190.3, 187.8, 188.1, 171.3, 189.9, 191.6, 188.0, 188.2, 198.8, 182.1],\n",
    " 'cp2': [264.7, 291.9, 244.0, 258.5, 204.5, 170.6, 237.0, 190.9, 160.2, 223.7],\n",
    " 'cp3': [280.5, 311.7, 247.8, 201.5, 167.2, 172.7, 225.2, 219.0, 174.4, 221.3],\n",
    " 'cp4': [291.7, 315.6, 196.7, 157.1, 144.3, 192.9, 271.9, 231.5, 136.7, 198.1],\n",
    " 'cp5': [285.8, 258.5, 204.3, 227.9, 144.4, 122.9, 258.3, 179.3, 127.1, 197.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_32m3cp = {'cp1': 180.37999999999997,\n",
    " 'cp2': 222.05,\n",
    " 'cp3': 217.65,\n",
    " 'cp4': 221.17999999999998,\n",
    " 'cp5': 221.26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_16m6cp = {'cp1': 187.60999999999999,\n",
    " 'cp2': 224.6,\n",
    " 'cp3': 222.13000000000002,\n",
    " 'cp4': 213.65,\n",
    " 'cp5': 200.64000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_random1 = {'cp1': [49.9, 65.7, 44.3, 42.1, 53.5, 37.3, 44.6, 68.9, 70.0, 27.0],\n",
    " 'cp2': [242.6, 166.4, 125.4, 53.4, 20.7, 51.1, 82.2, 59.2, 115.5, 98.6],\n",
    " 'cp3': [266.1, 106.6, 99.1, 49.6, 7.8, 37.5, 17.1, 63.8, 120.2, 66.8],\n",
    " 'cp4': [272.0, 85.1, 89.8, 55.4, 8.8, 32.5, 28.9, 38.1, 115.1, 87.2],\n",
    " 'cp5': [272.6, 166.8, 47.7, 67.3, 7.6, 22.5, 36.6, 23.4, 66.9, 106.2]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OvercookedAIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec7c984929472a33bd6cc10d53b9e778c56bf9f946e032cef7bc79990f4fb2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
